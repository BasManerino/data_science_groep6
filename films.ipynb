{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onderzoek Data Science: Films\n",
    "----\n",
    "\n",
    "Voor de casusopdracht van het vak Data Science moet er een pipeline worden gemaakt, waarbij een toegewezen dataset wordt geanalyseerd. Dit data onderzoek is gebaseerd op de dataset over films en is opgezet door groep 6 van klas V2C. De dataset is toegewezen door de Hogeschool Utrecht en bevat onder andere:\n",
    "- filmgegevens, waaronder duur, genres, taal, land van herkomst, budget en opbrengst;\n",
    "- likes op facebook voor regisseur, hoofdrolspelers, totale cast en de film zelf;\n",
    "- score op IMDB en aantal reviews. \n",
    "\n",
    "\n",
    "#### Hoofdvraag\n",
    "Voor dit onderzoek is er een verplichte onderzoeksvraag, die luidt als volgt: \n",
    "- In hoeverre is de omzet van een film te voorspellen op basis van de populariteit op Facebook en IMDB zelf?\n",
    "\n",
    "#### Deelvragen\n",
    "Het bevat een aantal onderzoeksvragen die wij zullen behandelen:\n",
    "\n",
    "|Beeordeling                    |Deelvraag                                                                             |\n",
    "|:------------------------------|:-------------------                                                                  |\n",
    "|Externe dataset                |Hoeveel effect heeft de lengte van een trailer op de omzet van de film?               |\n",
    "|Interactieve visualisatie      |Wat is de totale winst van alle films per jaar?                                       |\n",
    "|Correlatieonderzoek            |Is een film succesvoller op basis van het aantal likes die een acteur (of acteurs) heeft/hebben op facebook?                                                                                              | \n",
    "|Supervised Machine Learning    |In hoeverre is de omzet van een film te voorspellen op basis van de populariteit op Facebook en IMDB zelf?                                                                                                          |\n",
    "|Unsupervised Machine Learning  |Is een film succesvoller op basis van het aantal likes die een acteur (of acteurs) heeft/hebben op facebook?                                                                                              |\n",
    "\n",
    "#### Z-test\n",
    "Voor de hypothesetoets Z-test moet het volgende gebeuren.\t\n",
    "Een filmcriticus stelt dat de score van Engelstalige films lager is dan gemiddeld.\n",
    "Wij moeten onderzoeken met de dataset of deze filmcriticus gelijk heeft. We nemen een steekproef (met pandas.DataFrame.sample(n=100,random_state=1)) van 100 Engelstalige films en beschouwen de hele dataset als populatie. Ook nemen we als betrouwbaarheid 90%. We gebruiken van de dataset alleen de filmgegevens waarbij zowel de taal (language) als de score (imdb_score) bekend zijn.\n",
    "\n",
    "#### Teamleden\n",
    "Het team bestaat uit de volgende drie personen:\n",
    "- Sebastiaan Jansen\n",
    "- Skott de Koster\n",
    "- Mustafa Toprak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inhoudsopgave\n",
    "---\n",
    "1. Data Collection\n",
    "2. Data Processing\n",
    "3. Data Cleaning\n",
    "4. Data Exploration & Analysis\n",
    "5. Model building\n",
    "6. Visualization\n",
    "7. Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 1: Data collection\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De dataset is aan ons gegeven door de Hogeschool Utrecht als onderdeel van de opdracht. Deze dataset houd een groot CSV bestand in met informatie over films. We hebben dus nu de data verzameld en gaan het nu inlezen.\n",
    "De dataset is opgehaald van de volgende GitHub link: https://github.com/tijmenjoppe/ComputationalModelling-student/tree/master/casus/movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ook maken wij gebruik van een externe dataset. Deze externe dataset bevat een link ID naar de trailers van de films die op YouTube te vinden zijn. De externe dataset is opgehaald van de link: https://grouplens.org/datasets/movielens/20m-youtube/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 2: Data processing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gaan nu de dataset inlezen en verwerken om het beter te kunnen bekijken. Om te beginnen importeren we de benodigde Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deze libraries zijn voor het verwerken van de data.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Deze libraries zijn voor het verkrijgen van YouTube video gegevens. Dit is relevant voor het gebruik van de externe dataset.\n",
    "import pafy\n",
    "from youtube_dl import YoutubeDL\n",
    "import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vervolgens lezen we de eerste (toegewezen) dataset in. Hiervoor gebruiken we pandas.read_csv en slaan we dit op in de dataframe \"films\". \n",
    "Aangezien de dataset erg groot is, laten we voor nu alleen de eerste 5 rijen zien. Dit doen we door een variabele films.head() aan te roepen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films = pd.read_csv('movie.csv', sep=\",\")\n",
    "films.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hetzelfde doen we met de tweede (externe) dataset, deze noemen we \"films_extern\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_extern = pd.read_csv('ml-youtube.csv', sep=\",\")\n",
    "films_extern.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien dus dat de eerste dataset uit veel films bestaat. De tweede dataset heeft er nog veel meer, maar niet alle data in de sets is bruikbaar, dus dat gaan we nu schoonmaken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stap 3: Data cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gaan nu de data schoon maken. Om te beginnen schonen we beide datasets op door alle data die niet relevant is te verwijderen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['color','facenumber_in_poster','country','aspect_ratio','content_rating']\n",
    "films.drop(to_drop, inplace=True, axis=1)\n",
    "\n",
    "films_extern.drop('movieId', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We verwijderen de rijen met NaN gegevens in beide datasets, aangezien dit data is die we niet kunnen gebruiken.\n",
    "films = films.dropna()\n",
    "films_extern = films_extern.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We halen alle duplicates eruit.\n",
    "films = films.drop_duplicates(subset=\"movie_title\")\n",
    "films_extern = films_extern.drop_duplicates(subset=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We halen een whitespace weg in de eerste dataset die er niet in hoort te zitten.\n",
    "# We slicen de laatste 7 characters van de title column (het jaar dat de film uitkwam) in de tweede dataset weg,\n",
    "# zodat de column overeen komt met de column in de eerste dataset.\n",
    "films.update(films['movie_title'].str[:-1])\n",
    "films_extern.update(films_extern['title'].str[:-7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We setten de indexes van beide datasets naar de titel van de films.\n",
    "films.set_index('movie_title', inplace=True)\n",
    "films_extern.set_index('title', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sorteren beide datasets zodat de waardes gelijk zullen zijn.\n",
    "films = films.sort_index()\n",
    "films_extern = films_extern.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu ziet de eerste dataset er zo uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ziet de tweede dataset er zo uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_extern.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu gaan we beide datasets samenvoegen tot 1 dataframe, deze noemen we 'films_extern_merge'. Het idee is om de youtubeId column van de tweede dataset te verwerken in de eerste dataset. Deze gemergde dataset gaan we later gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omdat de youtubeId column het enige is wat relevant is, is dat ook het enige veld dat we toevoegen. \n",
    "# De indexes van de eerste en tweede dataset worden samengevoegd tot 1 index genaamd title.\n",
    "films_extern_merge = pd.merge(films, films_extern, left_index=True, right_index=True)\n",
    "films_extern_merge.index.name = 'title'\n",
    "\n",
    "# Tot slot droppen we de niet-relevante kollomen. Om code te besparen, overschrijven we alleen de dataframe met de relevante kollomen.\n",
    "films_extern_merge = films_extern_merge[['gross','youtubeId']]\n",
    "films_extern_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kunt zien dat er 2827 films zijn. Dit is bijna de helft minder dan er in de oorspronkelijke eerste dataset stond, dit is omdat we de onbruikbare data hebben verwijderd en er alleen bruikbare data is overgebleven.\n",
    "We gaan de gemergede dataset nu verkennen en analyseren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stap 4: Data exploration & analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de data te verkennen en er een betere grip op te krijgen zullen wij een paar commando's loslaten op de verwerkte data. Als eerste zullen wij een .describe gebruiken om een overzicht te krijgen van alle info van de numerieke waardes. Op deze manier kunnen wij bijvoorbeeld de gemiddelde lengte van een film zien. Dit is bij deze dataset dus 105 minuten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "films.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de data verder te verkennen en een beter gevoel te krijgen kunnen wij gebruik maken van grafieken. Zo staat hieronder bijvoorbeeld een staafdiagram van het aantal films per jaartal. In de x as staat het jaartal, deze zijn gesorteerd bij hoeveel films er in dat jaartal zijn uitgekomen die in deze dataset staan. De meeste films uit deze dataset komen dus uit 2002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "films['title_year'].value_counts().plot(kind='bar', width=0.5,figsize=(20,5));\n",
    "plt.xlabel(\"Jaartal\", labelpad=14)\n",
    "plt.ylabel(\"Aantal films\", labelpad=14)\n",
    "plt.title(\"Aantal films per jaartal\", y=1.02);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onderzoeks vragen\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor deze notebook moeten er verschillende onderzoeksvragen worden uitgewerkt, elk met hun eigen antwoord en andere manier van oplossen. Deze zullen nu 1 voor 1 worden uitgewerkt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vraag 1: Hoeveel effect heeft de lengte van een trailer op de omzet van de film?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor deze vraag maken we gebruik van de gemergde dataset uit stap 3. Zoals je misschien al hebt kunnen raden bestaat de externe dataset uit ID's voor YouTube video's. Deze verwijzen naar de trailers van de films van de toegewezen dataset. We hebben bij stap 3 de toegewezen en externe dataset gemerged tot 1 dataset, deze gaan we nu gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_extern_merge.iloc[:,-1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier komt de pafy library ter zake. Om een voorbeeld te geven over wat het kan doen maken we eerst een video variabele aan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Om het simpel te houden gebruiken we voor nu maar de youtubeId van de eerste film in de dataframe.\n",
    "url = films_extern_merge.iloc[0, -1]\n",
    "video = pafy.new(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Van deze video kunnen we nu verschillende data ophalen, hier een paar voorbeelden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De titel van de video\n",
    "print(\"De titel van de video is: \\n\" + str(video.title))\n",
    "print(\"\\n\")\n",
    "\n",
    "# De lengte van de video in secondes\n",
    "print(\"De lengte van de video in secondes is: \\n\" + str(video.length))\n",
    "print(\"\\n\")\n",
    "\n",
    "# De concrete duratie van de video\n",
    "print(\"De concrete duratie van de video is: \\n\" + str(video.duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er zijn nog veel andere statistieken die kunnen worden opgehaad met behulp van pafy, maar hiervoor is een YouTube API account voor nodig en is ook niet voor ons relevant. We focussen ons op de <b>concrete duratie</b> van de video.\n",
    "\n",
    "Dus, heeft de duratie van de trailer invloed op de omzet van de film? Om daar achter te komen maken we nieuwe dataframe aan, deze pakt 100 willekeurige films uit de dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# De dataframe heet \"random_films\".\n",
    "random_films = films_extern_merge.sample(n = 100)\n",
    "# In deze list slaan we zometeen de duraties op.\n",
    "duratie = []\n",
    "# We loopen door random_films heen en extracten van elke video de duratie in secondes.\n",
    "# LET OP! Dit duurt een tijdje, aangezien er elke keer opnieuw een HTTP request wordt gemaakt. Je kunt ook de errors en warnings negeren.\n",
    "i = 0\n",
    "for x in random_films['youtubeId'].values:\n",
    "    try:\n",
    "        duratie.append(pafy.new(x).length)\n",
    "        print(i)\n",
    "        i = i+1\n",
    "    except Exception:\n",
    "        # Sommige video's zijn niet meer te zien op YouTube, hier kan de data dus ook niet van worden opgehaald.\n",
    "        duratie.append(None)\n",
    "        print(i)\n",
    "        i = i+1\n",
    "        continue\n",
    "\n",
    "# We voegen een kolom toe met de waardes van duratie.\n",
    "random_films['trailer_length'] = duratie\n",
    "# We halen alle trailers eruit met een duratie van meer dan 400 seconden, aangezien deze vaak niet kloppen.\n",
    "random_films = random_films[random_films.trailer_length < 400]\n",
    "random_films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu maken we een mooie scatterplot om te zien of er een verband is tussen de omzet en trailer duratie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=random_films[\"trailer_length\"], y=random_films[\"gross\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals je kunt zien komen films qua omzet veel met elkaar overeen, ongeacht de lengte van de trailers.\n",
    "We kunnen dus concluderen dat er <b>geen</b> verband is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
