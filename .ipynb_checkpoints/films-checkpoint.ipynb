{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onderzoek Data Science: Films\n",
    "----\n",
    "\n",
    "Voor de casusopdracht van het vak Data Science moet er een pipeline worden gemaakt, waarbij een toegewezen dataset wordt geanalyseerd. Dit data onderzoek is gebaseerd op de dataset over films en is opgezet door groep 6 van klas V2C. De dataset is toegewezen door de Hogeschool Utrecht en bevat onder andere:\n",
    "- filmgegevens, waaronder duur, genres, taal, land van herkomst, budget en opbrengst;\n",
    "- likes op facebook voor regisseur, hoofdrolspelers, totale cast en de film zelf;\n",
    "- score op IMDB en aantal reviews. \n",
    "\n",
    "\n",
    "#### Hoofdvraag\n",
    "Voor dit onderzoek is er een verplichte onderzoeksvraag, die luidt als volgt: \n",
    "- In hoeverre is de omzet van een film te voorspellen op basis van de populariteit op Facebook en IMDB zelf?\n",
    "\n",
    "#### Onderzoeksvragen\n",
    "Dit onderzoek bevat een aantal onderzoeksvragen die wij zullen behandelen:\n",
    "\n",
    "|Beeordeling                    |Onderzoeksvraag                                                                       |\n",
    "|:------------------------------|:-------------------                                                                  |\n",
    "|Externe dataset                |Hoeveel effect heeft de lengte van een trailer op de omzet van de film?               |\n",
    "|Correlatieonderzoek            |Is een film succesvoller op basis van het aantal likes die een acteur (of acteurs) heeft/hebben op Facebook?                                                                                              | \n",
    "|Supervised machine learning    |In hoeverre is de omzet van een film te voorspellen op basis van de populariteit op Facebook en IMDB zelf?                                                                                                          |\n",
    "|Unsupervised machine learning  |Is een film succesvoller op basis van het aantal likes die een acteur (of acteurs) heeft/hebben op Facebook?                                                                                              |\n",
    "|Interactieve visualisatie      |Wat is de netto omzet van een film in het jaar van uitkomst?                          |\n",
    "\n",
    "#### Z-toets\n",
    "Voor de hypothesetoets Z-toets moet het volgende gebeuren.\t\n",
    "Een filmcriticus stelt dat de score van Engelstalige films lager is dan gemiddeld.\n",
    "Wij moeten onderzoeken met de dataset of deze filmcriticus gelijk heeft. We nemen een steekproef (met pandas.DataFrame.sample(n=100,random_state=1)) van 100 Engelstalige films en beschouwen de hele dataset als populatie. Ook nemen we als betrouwbaarheid 90%. We gebruiken van de dataset alleen de filmgegevens waarbij zowel de taal (language) als de score (imdb_score) bekend zijn.\n",
    "\n",
    "#### Teamleden\n",
    "Het team bestaat uit de volgende drie personen:\n",
    "- Sebastiaan Jansen\n",
    "- Skott de Koster\n",
    "- Mustafa Toprak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inhoudsopgave\n",
    "---\n",
    "1. Data Collection\n",
    "2. Data Processing\n",
    "3. Data Cleaning\n",
    "4. Data Exploration & Analysis\n",
    "5. Model building\n",
    "6. Visualization\n",
    "7. Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 1: Data collection\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De dataset is aan ons gegeven door de Hogeschool Utrecht als onderdeel van de opdracht. Deze dataset houd een groot CSV bestand in met informatie over films. We hebben dus nu de data verzameld en gaan het nu inlezen.\n",
    "De dataset is opgehaald van de volgende GitHub link: https://github.com/tijmenjoppe/ComputationalModelling-student/tree/master/casus/movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ook maken wij gebruik van een externe dataset. Deze externe dataset bevat een link ID naar de trailers van de films die op YouTube te vinden zijn. De externe dataset is opgehaald van de link: https://grouplens.org/datasets/movielens/20m-youtube/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 2: Data processing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gaan nu de dataset inlezen en verwerken om het beter te kunnen bekijken. Om te beginnen importeren we de benodigde Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deze libraries zijn voor het verwerken van de data.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as scipy\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "hv.extension(\"bokeh\")\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "\n",
    "# Deze libraries zijn voor het verkrijgen van YouTube video gegevens. Dit is relevant voor het gebruik van de externe dataset.\n",
    "import pafy\n",
    "from youtube_dl import YoutubeDL\n",
    "import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vervolgens lezen we de eerste (toegewezen) dataset in. Hiervoor gebruiken we pandas.read_csv en slaan we dit op in de dataframe \"films\". \n",
    "Aangezien de dataset erg groot is, laten we voor nu alleen de eerste 5 rijen zien. Dit doen we door een variabele films.head() aan te roepen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films = pd.read_csv(\"movie.csv\", sep=\",\")\n",
    "films.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hetzelfde doen we met de tweede (externe) dataset, deze noemen we \"films_extern\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_extern = pd.read_csv(\"ml-youtube.csv\", sep=\",\")\n",
    "films_extern.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien dus dat de eerste dataset uit veel films bestaat. De tweede dataset heeft er nog veel meer, maar niet alle data in de sets is bruikbaar, dus dat gaan we nu schoonmaken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stap 3: Data cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gaan nu de data schoon maken. Om te beginnen schonen we beide datasets op door alle data die niet relevant is te verwijderen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\"color\", \"facenumber_in_poster\", \"country\", \"content_rating\"]\n",
    "films.drop(to_drop, inplace=True, axis=1)\n",
    "\n",
    "films_extern.drop(\"movieId\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We verwijderen de rijen met NaN gegevens in beide datasets, aangezien dit data is die we niet kunnen gebruiken.\n",
    "films = films.dropna()\n",
    "films_extern = films_extern.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We halen alle duplicates eruit.\n",
    "films = films.drop_duplicates(subset=\"movie_title\")\n",
    "films_extern = films_extern.drop_duplicates(subset=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We halen een whitespace weg in de eerste dataset die er niet in hoort te zitten.\n",
    "# We slicen de laatste 7 characters van de title column (het jaar dat de film uitkwam) in de tweede dataset weg,\n",
    "# zodat de column overeen komt met de column in de eerste dataset.\n",
    "films.update(films[\"movie_title\"].str[:-1])\n",
    "films_extern.update(films_extern[\"title\"].str[:-7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We setten de indexes van beide datasets naar de titel van de films.\n",
    "films.set_index(\"movie_title\", inplace=True)\n",
    "films_extern.set_index(\"title\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sorteren beide datasets zodat de waardes gelijk zullen zijn.\n",
    "films = films.sort_index()\n",
    "films_extern = films_extern.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu ziet de eerste dataset er zo uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ziet de tweede dataset er zo uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_extern.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu gaan we beide datasets samenvoegen tot 1 dataframe, deze noemen we 'films_extern_merge'. Het idee is om de youtubeId column van de tweede dataset te verwerken in de eerste dataset. Deze gemergde dataset gaan we later gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omdat de youtubeId column het enige is wat relevant is, is dat ook het enige veld dat we toevoegen. \n",
    "# De indexes van de eerste en tweede dataset worden samengevoegd tot 1 index genaamd title.\n",
    "films_extern_merge = pd.merge(films, films_extern, left_index=True, right_index=True)\n",
    "films_extern_merge.index.name = \"title\"\n",
    "\n",
    "# Tot slot droppen we de niet-relevante kollomen. Om code te besparen, overschrijven we alleen de dataframe met de relevante kollomen.\n",
    "films_extern_merge = films_extern_merge[[\"gross\", \"youtubeId\"]]\n",
    "films_extern_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kunt zien dat er nu 2827 films zijn. Dit is bijna de helft minder dan er in de oorspronkelijke eerste dataset stond, dit is omdat veel films geen trailer op YouTube hebben staan. We zullen deze gemergede dataset uiteindelijk gebruiken, maar we zullen het meest de normale dataset gebruiken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stap 4: Data exploration & analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de data te verkennen en er een betere grip op te krijgen zullen wij een paar commando's loslaten op de verwerkte data. Als eerste zullen wij een .describe gebruiken om een overzicht te krijgen van alle info van de numerieke waardes. Op deze manier kunnen wij bijvoorbeeld de gemiddelde lengte van een film zien. Dit is bij deze dataset dus 105 minuten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "films.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de data verder te verkennen en een beter gevoel te krijgen kunnen wij gebruik maken van grafieken. Zo staat hieronder bijvoorbeeld een staafdiagram van het aantal films per jaartal. In de x as staat het jaartal, deze zijn gesorteerd bij hoeveel films er in dat jaartal zijn uitgekomen die in deze dataset staan. De meeste films uit deze dataset komen dus uit 2002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "films[\"title_year\"].value_counts().plot(kind=\"bar\", width=0.5,figsize=(20,5));\n",
    "plt.xlabel(\"Jaartal\", labelpad=14)\n",
    "plt.ylabel(\"Aantal films\", labelpad=14)\n",
    "plt.title(\"Aantal films per jaartal\", y=1.02);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onderzoeksvragen\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor deze notebook moeten er verschillende onderzoeksvragen worden uitgewerkt, elk met hun eigen antwoord en andere manier van oplossen. Deze zullen nu één voor één worden uitgewerkt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vraag 1: Hoeveel effect heeft de lengte van een trailer op de omzet van de film?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trailers zijn een belangrijk onderdeel van films. Je wilt immers wel enigzins weten wat je kan verwachten van de film.<br>\n",
    "Dit kan ook een factor spelen in de omzet van de film. Want als de trailer slecht is zal je waarschijnlijk ook niet snel naar de film gaan, en dan zal de film dus ook minder omzet hebben.<br>\n",
    "We gebruiken hier alleen de lengte van de trailer, om het simpel te houden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor deze vraag maken we gebruik van de gemergde dataset uit stap 3. Zoals je misschien al hebt kunnen raden bestaat de externe dataset uit ID's voor YouTube video's. Deze verwijzen naar de trailers van de films van de toegewezen dataset. We hebben bij stap 3 de toegewezen en externe dataset gemerged tot 1 dataset, deze gaan we nu gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_extern_merge.iloc[:,-1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier komt de pafy library ter zake. Om een voorbeeld te geven over wat het kan doen maken we eerst een video variabele aan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Om het simpel te houden gebruiken we voor nu maar de youtubeId van de eerste film in de dataframe.\n",
    "url = films_extern_merge.iloc[0, -1]\n",
    "video = pafy.new(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Van deze video kunnen we nu verschillende data ophalen, hier een paar voorbeelden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De titel van de video\n",
    "print(\"De titel van de video is: \\n\" + str(video.title))\n",
    "print(\"\\n\")\n",
    "\n",
    "# De lengte van de video in secondes\n",
    "print(\"De lengte van de video in secondes is: \\n\" + str(video.length))\n",
    "print(\"\\n\")\n",
    "\n",
    "# De concrete duratie van de video\n",
    "print(\"De concrete duratie van de video is: \\n\" + str(video.duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er zijn nog veel andere statistieken die kunnen worden opgehaad met behulp van pafy, maar hiervoor is een YouTube API account voor nodig en is ook niet voor ons relevant. We focussen ons op de <b>concrete duratie</b> van de video.\n",
    "\n",
    "Dus, heeft de duratie van de trailer invloed op de omzet van de film? Om daar achter te komen maken we nieuwe dataframe aan, deze pakt 100 willekeurige films uit de dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# De dataframe heet \"random_films\".\n",
    "random_films = films_extern_merge.sample(n = 100)\n",
    "# In deze list slaan we zometeen de duraties op.\n",
    "duratie = []\n",
    "# We loopen door random_films heen en extracten van elke video de duratie in secondes.\n",
    "# LET OP! Dit duurt een tijdje, aangezien er elke keer opnieuw een HTTP request wordt gemaakt. Je kunt ook de errors en warnings negeren.\n",
    "for x in random_films[\"youtubeId\"].values:\n",
    "    try:\n",
    "        # We appenden de lengte van de trailer toe aan de list.\n",
    "        duratie.append(pafy.new(x).length)\n",
    "    except Exception:\n",
    "        # Sommige video's zijn niet meer te zien op YouTube, hier kan de data dus ook niet van worden opgehaald.\n",
    "        # Maar aangezien de waardes niet door elkaar mogen lopen, appenden we een Null waarde.\n",
    "        duratie.append(None)\n",
    "        continue\n",
    "\n",
    "# We voegen een kolom toe met de waardes van duratie.\n",
    "random_films[\"trailer_length\"] = duratie\n",
    "# We halen alle trailers eruit met een duratie van meer dan 400 seconden, aangezien deze vaak niet kloppen.\n",
    "random_films = random_films[random_films.trailer_length < 400]\n",
    "random_films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu maken we een mooie scatterplot om te zien of er een verband is tussen de omzet en trailer duratie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=random_films[\"trailer_length\"], y=random_films[\"gross\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elke uitkomst die je krijgt zal een ander resultaat geven. Maar wat wel duidelijk is in elke uitkomst is dat er geen specifiek partroon te zien is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We kunnen dus concluderen dat er **geen** duidelijk verband is tussen de lengte van de trailer en de omzet van de film."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vraag 2: Is een film succesvoller op basis van het aantal likes die een acteur (of acteurs) heeft/hebben op Facebook?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social media speelt tot op de dag van vandaag een hele belangrijke rol in ons dagelijks leven. Overal zien we beroemdheden en sommige mensen willen heel graag op deze beroemdheden lijken. Hierdoor wordt het gedrag van een individu bepaald. \n",
    "\n",
    "Tegenwoordig draaien de meeste dingen om likes en dat geldt ook voor deze deelvraag. Is het echt waar dat een film succesvoller is op basis van het aantal likes die een acteur (of acteurs) heeft/hebben op Facebook?\n",
    "\n",
    "Om die vraag te kunnen beantwoorden, moeten we eerst een beoordelingscriterium langs. Dit criterium zal gaan over het correlatieonderzoek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlatieonderzoek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met behulp van correlatie wordt de samenhang gemeten tussen twee variabelen. \n",
    "\n",
    "Het is belangrijk dat je de waarde van een andere variabele goed kunt voorspellen o.b.v. waardes van een variabele die je al kent. Hoe groter de correlatie, des te hoger de voorspellingskracht is.<br>\n",
    "Om deze vraag te kunnen beantwoorden, worden de variabelen IMDB-score en het gemiddelde aantal likes van de acteurs genomen.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder maken we eerst een extra kolom aan die het gemiddelde neemt van het aantal likes van de acteurs. Deze kolom voegen we toe aan de dataset films:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Voeg een kolom van het gemiddeld aantal likes van de acteurs toe aan de dataset films.\n",
    "films[\"mean_likes_actors\"] = (films[\"actor_1_facebook_likes\"] + \n",
    "                              films[\"actor_2_facebook_likes\"] + \n",
    "                              films[\"actor_3_facebook_likes\"]) / 3\n",
    "films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals gezegd hebben de worden de variabelen IMDB-score en mean_likes_actors gebruikt. Hieronder wordt een scatter-plot gemaakt van deze variabelen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak gebruik van de Seaborn style.\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "# Maak een scatter-plot van de variabelen.\n",
    "plt.scatter(films.imdb_score, films.mean_likes_actors, \n",
    "            s=40, \n",
    "            c=\"red\", edgecolor=\"black\", \n",
    "            linewidth=0.75, alpha=0.75)\n",
    "\n",
    "# Voeg kenmerken toe aan de plot.\n",
    "plt.xlabel(\"IMDB-score\")\n",
    "plt.ylabel(\"Aantal likes\")\n",
    "plt.title(\"Correlatie tussen IMDB-score en aantal likes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals te zien is liggen de punten heel dicht op elkaar. Nu laten we nog even de r-waardes (richtingscoëfficient) zien van de correlatie tussen deze twee variabelen. Hiervoor maken we een aparte dataset aan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Voeg de kolommen imdb_score en mean_likes_actors toe aan een nieuwe dataset.\n",
    "films_correlation = films[[\"imdb_score\", \"mean_likes_actors\"]]\n",
    "films_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toon de correlatie tussen de twee variabelen.\n",
    "films_correlation.corr().style.background_gradient(cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We hebben een aparte dataset aangemaakt voor deze correlatie, omdat de andere gegevens niet relevant zijn voor het beantwoorden van de vraag. Mocht u geïntereseerd zijn in de correlatie met alle variabelen, kunt u het onderstaande statement runnen:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geef een overzicht van de correlatie met alle variabelen.\n",
    "# films.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor een goede correlatie is het belangrijk dat de richtingscoëfficient dicht bij de 1 zit ($ -1 ≤ r ≤ 1$). We zien uit de bovenstaande gegevens dat de richtingscoëfficient voor imdb_score en mean_likes_actors 0.187995 is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er is sprake van een positieve correlatie, maar er is **geen** sprake van een sterk verband tussen het succes van een film en het aantal likes van acteurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vraag 3: In hoeverre is de omzet van een film te voorspellen op basis van de populariteit op Facebook en IMDB zelf?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omzet is erg belangrijk voor een film. Het is immers waar een film voor wordt gemaakt: om omzet te draaien. Deze omzet komt natuurlijk van alle mensen die deze film hebben gezien in de bioscoop en uiteindelijk ook de DVD's. <br>\n",
    "Maar natuurlijk gaat de gemiddelde persoon niet naar elke film die ooit uitkomt. Als een film een lage score heeft op IMDB of weinig Facebook likes heeft, geeft dat al gauw een signaal dat de film niet zo goed is. Het is dus veilig om te zeggen dat zo'n score invloed heeft op de populariteit van een film.\n",
    "\n",
    "Maar heeft dit ook veel effect op de omzet van de film? En tot hoeverre kunnen we hiermee de omzet van toekomstige films voorspellen? Het criterium dat valt onder deze vraag is **supervised machine learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised machine learning is gebaseerd op een dataset waarvan de uitkomst al bekend is (*labeled*). Dit houdt in dat we de machine letterlijk \"begeleiden\" in zijn taak om iets uit te voeren. We geven de machine data om te onderzoeken en zorgen dat we deze een goed antwoord kan geven.<br>\n",
    "Onder supervised machine learning worden er binnen het vak Data Science twee technieken verstaan:\n",
    "    \n",
    "|Techniek                       |Omschrijving                          |                                \n",
    "|:------------------------------|:-------------------------------------|\n",
    "|Lineaire regressie             |Lineaire regressie probeert de relatie tussen twee variabelen te modelleren door een **lineaire vergelijking** aan te passen aan de waargenomen gegevens.   |\n",
    "|Classificatie                  |Classificatie is een geordende reeks gerelateerde categorieën die wordt gebruikt om gegevens te **groeperen op basis van overeenkomsten**. Binnen het vak Data Science bevat het 2 onderdelen: **Decision Trees** en **k-Nearest Neighbours**.                                                  |\n",
    "\n",
    "Om een betrouwbaar onderzoek uit te voeren maken we gebruik van <b>beide</b> technieken. We beginnen met lineaire regressie, dan classificatie. Maar voordat we beginnen moeten we een paar dingen instellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We maken een kopie van de films dataset met alleen de relevante data.\n",
    "films_sml = films[[\"imdb_score\", \"movie_facebook_likes\", \"gross\"]].copy()\n",
    "\n",
    "# We stellen de variabele X op, deze vertegenwoordigd de feature matrix (X-as).\n",
    "# Het bevat de totale filmscore op IMDB en de het totale aantal facebook likes per film.\n",
    "feature_cols = [\"imdb_score\",\"movie_facebook_likes\"]\n",
    "\n",
    "X = films_sml[feature_cols]\n",
    "\n",
    "# We stellen we de variabele Y op, deze vertegenwoordigd de target vector(Y-as).\n",
    "# Het bevat de totale omzet per film.\n",
    "Y = films_sml.gross\n",
    "\n",
    "# We splitten de data op in test en training sets. 75% van de data gaat naar de training set, 25% gaat naar de test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=1)\n",
    "\n",
    "# We printen de shape van de training sets.\n",
    "print(\"De shape van de X train set is \" + str(X_train.shape))\n",
    "print(\"De shape van de Y train set is \" + str(Y_train.shape))\n",
    "print(\"De shape van de X test set is \" + str(X_test.shape))\n",
    "print(\"De shape van de Y test set is \" + str(Y_test.shape))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principe hebben we nu de eerste paar stappen van de werkwijze van Scikit-learn toegespast. De variabelen kunnen we hergebruiken in beide technieken. Zo hoeven we ze niet telkens opnieuw aan te maken.<br>\n",
    "Nu we de voorbereidingen hebben gemaakt, kunnen we beginnen met lineaire regressie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We importeren het benodigde onderdeel van scikit-learn.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# We instantieeren de lineaire regressie.\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# We fitten het model aan de training data, oftewel we leren de coëfficienten.\n",
    "linreg.fit(X_train, Y_train)\n",
    "\n",
    "# We printen the onderschepping en coëfficiënten.\n",
    "print(\"De onderschepping van de lineaire regressie is \" + str(linreg.intercept_))\n",
    "print(\"De coëfficiënt van de lineaire regressie is \" + str(linreg.coef_))\n",
    "print(\"\\n\")\n",
    "\n",
    "# We paren de coëfficiënten met de feature namen.\n",
    "list(zip(feature_cols, linreg.coef_))\n",
    "\n",
    "# We maken voorspellingen op de test set.\n",
    "Y_pred = linreg.predict(X_test)\n",
    "\n",
    "# Tot slot berekenen we de RMSE (Root Mean Squared Error) voor de omzet voorspellingen.\n",
    "print(\"De RMSE van de omzet voorspellingen is \" + str(np.sqrt(metrics.mean_squared_error(Y_test, Y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze nummers zeggen niet veel duidelijks. We willen de RMSE namelijk zo laag mogelijk hebben. Dit kunnen we proberen om waar te maken door de code opnieuw te draaien, maar met 1 feature in plaats van 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er is hier een mogelijk verklaring voor: <b>correlatie.</b> Je hebt immers voor lineaire regressie enige <b>correlatie nodig</b>. Om te checken of er correlatie is doen we het volgende:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sml.corr().style.background_gradient(cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien dat de correlatie tussen de feature matrix en target vector erg klein is. Voor het correct uitvoeren van lineaire regressie is een minimale correlatie nodig van <b>0.7</b> (positief) of <b>-0.7</b> (negatief)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als we nu een aantal scatterplots opstellen tussen de feature matrix en target vector kunnen we zien waarom er weinig correlatie is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(films_sml, x_vars=[\"imdb_score\",\"movie_facebook_likes\"], y_vars=\"gross\", height=7, aspect=0.9, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In beide scatterplots is geen goede correlatie te zien. We kunnen nu dus concluderen dat er met lineaire regressie <b>geen</b> goede voorspelling kan worden gemaakt. <br>\n",
    "\n",
    "Maar hoe zit het met classificatie? Dat gaan we nu uitzoeken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals eerder vermeld vallen er binnen het vak Data Science 2 technieken onder classificatie: **Decision Trees** en **k-Nearest Neighbours**. Binnen deze context kunnen we alleen gebruik maken van Decision Trees. De techniek k-Nearest Neighbours is namelijk voor het indelen van een **nieuw object**, wat in principe niet kan bijdragen aan het beantwoorden van de vraag.\n",
    "\n",
    "Decision trees is een algoritme dat telkens een vraag bedenkt om de bestaande objecten in te delen in groepen, totdat deze uiteindelijk nauwkeurig zijn ingedeeld. Er wordt mee voorspeld wat het type is van verschillende cultivars, bijvoorbeeld een appel als je fruit gaat onderzoeken.<br>\n",
    "In dit geval zal het proberen om de types IMDB score of facebook likes te vinden.<br>\n",
    "Laten we beginnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We importeren het benodigde onderdeel van scikit-learn.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# We instantieeren de decision tree classifier.\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# We fitten de decision tree op de training sets.\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "\n",
    "# We voorspellen hoe nauwkeurig het algoritme de type van de cultivars kan voorspellen.\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(\"Nauwkeurigheid:\", metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er is een 0.0 in nauwkeurigheid. Dit is heel slecht, want dit laat zien dat het algoritme de types absoluut niet kan voorspellen. Dit kunnen we vaststellen door Graphviz te gebruiken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dit is een fix voor een error met graphviz.\n",
    "# Het zorgt ervoor dat de error \"InvocationException: GraphViz's executables not found\" niet voorkomt.\n",
    "import os\n",
    "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \";\" + os.environ[\"CONDA_PREFIX\"] + r\"\\Library\\bin\\graphviz\"\n",
    "\n",
    "# We importeren de benodigde libraries.\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG\n",
    "\n",
    "# We genereren een graph met de decision tree en visualeren die.\n",
    "graph = Source(tree.export_graphviz(clf, out_file=None, feature_names=X.columns))\n",
    "SVG(graph.pipe(format=\"svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze graphic is absoluut veel te groot en je wordt er niet wijzer uit. We kunnen dus concluderen dat ook met decision trees de omzet **niet goed voorspeld kan worden.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wat kan de reden zijn dat de omzet niet goed voorspeld kan worden? Als we nog een keer naar de scatterplots kijken..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(films_sml, x_vars=[\"imdb_score\",\"movie_facebook_likes\"], y_vars=\"gross\", height=7, aspect=0.9, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... dan zien we dat imdb_score en movie_facebook_likes compleet andere eigenschappen hebben. De IMDB score is een getal van 1 tot 10, terwijl de facebook likes kunnen lopen tot in de 300.000. De 2 variabelen verschillen te veel van elkaar en hebben te weinig correlatie voor een goede voorspelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maar we zijn nog niet klaar. We hebben nu de decision trees en lineaire regressie toegepast op niet-bruikbare data, maar wat als we proberen om het wel bruikbaar te maken? We gaan nu helemaal opnieuw beginnen, maar deze keer gebruiken we een **z-score** van de vorige feature matrix als nieuwe feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We maken een kopie van de films dataset met alleen de relevante data.\n",
    "films_sml = films[[\"imdb_score\", \"movie_facebook_likes\", \"gross\"]].copy()\n",
    "\n",
    "# Om de z-score toe te passen, voegen we 2 kollomen toe. Deze kollomen bevatten de z-score van hun respectievelijke kollomen.\n",
    "cols = list(films_sml.columns)\n",
    "cols.remove(\"gross\")\n",
    "cols.remove(\"imdb_score\")\n",
    "\n",
    "for col in cols:\n",
    "    col_zscore = col + \"_zscore\"\n",
    "    films_sml[col_zscore] = (films_sml[col] - films_sml[col].mean()) / films_sml[col].std()\n",
    "\n",
    "films_sml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hebben nu Facebook likes gestandaardiseerd aan de hand van een z-score. We hebben dit niet uitgevoerd op de imdb_score aangezien dat al een prima score geeft.<br>\n",
    "Zal dit iets veranderen? Laten we kijken in een scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(films_sml, x_vars=[\"imdb_score\",\"movie_facebook_likes_zscore\"], y_vars=\"gross\", height=7, aspect=0.9, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals we kunnen zien is er niets tot bijna niets veranderd. De data is nog steeds verwarrend en er zit nog steeds weinig correlatie in. Krijgen we nu ook ongeveer dezelfde uitkomst bij lineaire regressie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We stellen de variabele X op, deze vertegenwoordigd de feature matrix (X-as).\n",
    "# Het bevat de totale filmscore op IMDB en de z-score van het totale aantal facebook likes per film.\n",
    "feature_cols = [\"imdb_score\",\"movie_facebook_likes_zscore\"]\n",
    "\n",
    "X = films_sml[feature_cols]\n",
    "\n",
    "# We stellen we de variabele Y op, deze vertegenwoordigd de target vector(Y-as).\n",
    "# Het bevat de totale omzet per film.\n",
    "Y = films_sml.gross\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=1)\n",
    "\n",
    "# We fitten het model aan de training data, oftewel we leren de coeffienten.\n",
    "linreg.fit(X_train, Y_train)\n",
    "\n",
    "# We paren de coëfficiënten met de feature namen.\n",
    "list(zip(feature_cols, linreg.coef_))\n",
    "\n",
    "# We maken voorspellingen op de test set.\n",
    "Y_pred = linreg.predict(X_test)\n",
    "\n",
    "# Tot slot berekenen we de RMSE (Root Mean Squared Error) voor de omzet voorspellingen.\n",
    "print(\"De RMSE van de omzet voorspellingen is \" + str(np.sqrt(metrics.mean_squared_error(Y_test, Y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De RMSE is niets veranderd. Maar wat als we 1 van de features verwijderen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We stellen de variabele X op, deze vertegenwoordigd de feature matrix (X-as).\n",
    "# Het bevat nu alleen de totale filmscore op IMDB.\n",
    "feature_cols = [\"imdb_score\"]\n",
    "\n",
    "X = films_sml[feature_cols]\n",
    "\n",
    "# We stellen we de variabele Y op, deze vertegenwoordigd de target vector(Y-as).\n",
    "# Het bevat de totale omzet per film.\n",
    "Y = films_sml.gross\n",
    "\n",
    "# We splitten de data op in test en training sets. 75% van de data gaat naar de training set, 25% gaat naar de test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=1)\n",
    "\n",
    "# We fitten het model aan de training data, oftewel we leren de coeffienten.\n",
    "linreg.fit(X_train, Y_train)\n",
    "\n",
    "# We paren de coëfficiënten met de feature namen.\n",
    "list(zip(feature_cols, linreg.coef_))\n",
    "\n",
    "# We maken voorspellingen op de test set.\n",
    "Y_pred = linreg.predict(X_test)\n",
    "\n",
    "# Tot slot berekenen we de RMSE (Root Mean Squared Error) voor de omzet voorspellingen.\n",
    "print(\"De RMSE van de omzet voorspellingen is \" + str(np.sqrt(metrics.mean_squared_error(Y_test, Y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De RMSE is gestegen. Dit willen we niet, we willen juist dat het wordt verlaagd. Wat als we alleen de z-score van de Facebook likes gebruiken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We stellen de variabele X op, deze vertegenwoordigd de feature matrix (X-as).\n",
    "# Het bevat nu alleen de totale filmscore op IMDB.\n",
    "feature_cols = [\"movie_facebook_likes_zscore\"]\n",
    "\n",
    "X = films_sml[feature_cols]\n",
    "\n",
    "# We stellen we de variabele Y op, deze vertegenwoordigd de target vector(Y-as).\n",
    "# Het bevat de totale omzet per film.\n",
    "Y = films_sml.gross\n",
    "\n",
    "# We splitten de data op in test en training sets. 75% van de data gaat naar de training set, 25% gaat naar de test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=1)\n",
    "\n",
    "# We fitten het model aan de training data, oftewel we leren de coeffienten.\n",
    "linreg.fit(X_train, Y_train)\n",
    "\n",
    "# We paren de coëfficiënten met de feature namen.\n",
    "list(zip(feature_cols, linreg.coef_))\n",
    "\n",
    "# We maken voorspellingen op de test set.\n",
    "Y_pred = linreg.predict(X_test)\n",
    "\n",
    "# Tot slot berekenen we de RMSE (Root Mean Squared Error) voor de omzet voorspellingen.\n",
    "print(\"De RMSE van de omzet voorspellingen is \" + str(np.sqrt(metrics.mean_squared_error(Y_test, Y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ook hier is de RMSE gestegen, al is het minder gestegen dan toen de IMDB score weghaalden. De RMSE is dus het laagst als we de IMDB score en Facebook likes allebei implementeren, maar het is nog steeds heel hoog en we kunnen er geen voorspelling uit trekken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zijn nu klaar met lineaire regressie. We kunnen eruit concluderen dat het simpelweg niet mogelijk is om een goede voorspelling te trekken uit lineaire regressie, want er is te weinig correlatie en de data verschilt te veel van elkaar, ook al is het gestandaardiseerd met een z-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gaan het nu nog 1 keer proberen met decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We instantieeren de decision tree classifier.\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# We fitten de decision tree op de training sets.\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "\n",
    "# We voorspellen hoe nauwkeurig het algoritme de type van de cultivars kan voorspellen.\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(\"Nauwkeurigheid:\", metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We krijgen weer een nauwkeurigheid van 0.0, wat betekent dat het algoritme nog steeds geen onderscheidt kan maken tussen de objecten, ook al hebben we de z-score toegepast. Als we nu Graphfiz gebruiken zullen een (bijna) identieke tree krijgen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We genereren een graph met de decision tree en visualeren die.\n",
    "graph = Source(tree.export_graphviz(clf, out_file=None, feature_names=X.columns))\n",
    "SVG(graph.pipe(format=\"svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We kunnen nu dus concluderen dat ook met de z-score, de decision trees ook niet kunnen worden toegepast. De features verschillen teveel van elkaar voor een duidelijke vergelijking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De omzet kan **niet** goed worden voorspeld aan de hand van de IMDB score en likes op Facebook met supervised machine learning, omdat de features te veel van elkaar verschillen en er te weinig correlatie is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vraag 4 ...........\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het criterium dat valt onder deze vraag is **unsupervised machine learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised machine learning is gebaseerd op datasets zonder specifieke uitkomst (*unlabeled*). Hierbij probeer je groepen/indelingen te vinden die nuttig zijn (*pattern recognition*). Onder supervised machine learning worden er binnen het vak Data Science twee technieken verstaan:\n",
    "\n",
    "|Techniek                       |Omschrijving                          |                                \n",
    "|:------------------------------|:-------------------------------------|\n",
    "|Clustering                     |Het doel van clustering is om vanuit de kenmerken zelf groepen te laten vormen. Binnen het vak Data Science valt het onder twee onderdelen: **k-Means** en **Gaussian Mixture Model**|\n",
    "|Dimensionality Reduction       |Het doel van dimensionality reduction is eerst de dataset vereenvoudingen door het verminderen of samenvoegen van kenmerken; en dan groepen laten vormen. Deze valt buiten de scope van het vak.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor het gebruik van unsupervised machine learning moeten we dus clustering toepassen. De techniek die we gaan gebruiken is k-Means. Hiervoor is gekozen omdat k-Means een veelgebruikte en eenvoudig te begrijpen clustering techniek is. Je zoekt *k* verchillende clusters in een verzameling gegevens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> N.B.: Eerst maken we een scatter-plot van het aantal facebook likes van de film tegen andere numerieke kolommen, zodat we clusters kunnen vinden voor onze nieuwe onderzoeksvraag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-plot tussen movie_facebook_likes en num_critic_for_reviews\n",
    "films.plot.scatter(x=\"movie_facebook_likes\", y=\"num_critic_for_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-plot tussen movie_facebook_likes en duration\n",
    "films.plot.scatter(x=\"movie_facebook_likes\", y=\"duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-plot tussen movie_facebook_likes en director_facebook_likes\n",
    "films.plot.scatter(x=\"movie_facebook_likes\", y=\"director_facebook_likes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-plot tussen movie_facebook_likes en actor_1_facebook_likes\n",
    "films.plot.scatter(x=\"movie_facebook_likes\", y=\"actor_1_facebook_likes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-plot tussen movie_facebook_likes en actor_2_facebook_likes\n",
    "films.plot.scatter(x=\"movie_facebook_likes\", y=\"actor_2_facebook_likes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-plot tussen movie_facebook_likes en actor_3_facebook_likes\n",
    "films.plot.scatter(x=\"movie_facebook_likes\", y=\"actor_3_facebook_likes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-plot tussen movie_facebook_likes en gross\n",
    "films.plot.scatter(x=\"movie_facebook_likes\", y=\"gross\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-plot tussen movie_facebook_likes en num_voted_users\n",
    "films.plot.scatter(x=\"movie_facebook_likes\", y=\"num_voted_users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-plot tussen movie_facebook_likes en num_user_for_reviews\n",
    "films.plot.scatter(x=\"movie_facebook_likes\", y=\"num_user_for_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-plot tussen movie_facebook_likes en budget\n",
    "films.plot.scatter(x=\"movie_facebook_likes\", y=\"budget\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-plot tussen movie_facebook_likes en title_year\n",
    "films.plot.scatter(x=\"movie_facebook_likes\", y=\"title_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-plot tussen movie_facebook_likes en imdb_score\n",
    "films.plot.scatter(x=\"movie_facebook_likes\", y=\"imdb_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-plot tussen movie_facebook_likes en mean_likes_actors\n",
    "films.plot.scatter(x=\"movie_facebook_likes\", y=\"mean_likes_actors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Maak hier een scatter-plot van andere kolommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-plot tussen aspect_ratio en gross\n",
    "films.plot.scatter(x=\"aspect_ratio\", y=\"gross\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-toets\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een filmcriticus heeft een hypothese: **de score van Engelstalige films is lager dan gemiddeld.**<br>\n",
    "Is deze hypothese correct? Dat gaan we uitzoeken aan de hand van een z-toets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De bekende gegevens zijn als volgt:\n",
    "- De steekproef (n) is 100 films.\n",
    "- De betrouwbaarheid van de steekproef is 90%.\n",
    "- De foutmarge (a) van de steekproef is 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laten we beginnen.<br>\n",
    "Eerst stellen we 2 hypotheses op:\n",
    "- **H0** = µ_engelstalige films >= gemiddelde score\n",
    "- **HA** = µ_engelstalige films < gemiddelde \n",
    "<br>\n",
    "\n",
    "We gaan nu berekenen welke hypothese we kunnen accepteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We importeren de benodigde libraries.\n",
    "from scipy.stats import norm as norm\n",
    "\n",
    "# We stellen de foutmarge in.\n",
    "a = 0.1\n",
    "print(\"De foutmarge van de steekproef is {}.\".format(a))\n",
    "\n",
    "# We nemen het gemiddelde van de populatie.\n",
    "µ = round(films[\"imdb_score\"].mean(), 1)\n",
    "print (\"Het gemiddelde van de populatie is {}.\".format(µ)) \n",
    "\n",
    "# We maken de steekproef.\n",
    "sample = films[films[\"imdb_score\"] != None][films[\"language\"] == \"English\"].sample(n=100, random_state=1)\n",
    "\n",
    "# We berekenen het gemiddelde van de steekproef.\n",
    "x = round(sample[\"imdb_score\"].mean(), 1)\n",
    "print(\"Het gemiddelde van de steekproef is {}.\".format(x))\n",
    "\n",
    "# We berekenen de standaarddeviatie van de steekproef.\n",
    "s = sample[\"imdb_score\"].std()\n",
    "print(\"De standaarddeviatie van de steekproef is {}.\".format(s))\n",
    "                     \n",
    "# We berekenen de standard error.\n",
    "σx = s / np.sqrt(100)\n",
    "print(\"De standard error is {}.\".format(σx)) \n",
    "\n",
    "# We berekenen de Z-waarde.\n",
    "z = (x - µ) / σx\n",
    "print(\"De Z-waarde is {}.\".format(z)) \n",
    "\n",
    "# We berekenen de P-waarde.\n",
    "p = scipy.stats.norm.sf(z)\n",
    "print(\"De P-waarde is {}.\".format(p) + \"\\n\") \n",
    "\n",
    "# We kijken of de P-waarde meer of minder is dan de foutmarge.\n",
    "# Is het minder, dan wijzen we de nul hypothese af.\n",
    "# Is het meer of gelijk, dan accepteren we de nul hypothese.\n",
    "if p < a:\n",
    "    print(\"De P-waarde ({}) is kleiner dan de foutmarge ({}), dus we wijzen de nul hypothese af.\".format(p, a))\n",
    "elif p == a:\n",
    "    print(\"De P-waarde ({}) is gelijk aan de foutmarge ({}), dus we accepteren de nul hypothese.\".format(p, a))\n",
    "elif p > a:\n",
    "    print(\"De P-waarde ({}) is groter dan de foutmarge ({}), dus we accepteren de nul hypothese.\".format(p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien dat de P-waarde hoger is dan de foutmarge en we de nul hypothese accepteren. We blijven dus nog skeptisch over de alternatieve hypothese. Maar dit betekent niet dat de nul hypothese per se waar is, het betekent alleen dat we nog skeptisch zijn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stap 6. Visualization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze stap is om niet alleen voor onszelf, maar ook voor u als lezer, inzicht te krijgen in de data van onze Data Science pipeline. \n",
    "\n",
    "Dankzij de interactieve visualisate met Holoviews kunt u invloed uitoefenen op wat u te zien krijgt qua analyses. De bijbehorende vraag bij deze stap is: Wat is de totale winst van alle films in een bepaald jaar? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vraag 5: Wat is de netto omzet van een film in het jaar van uitkomst?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het is voor een regisseur uiterst belangrijk om zoveel mogelijk geld te verdienen wanneer hij zijn film uitbrengt. Geld speelt immers een grote rol in ons leven. \n",
    "\n",
    "Daarom laten wij u nu zien wat de netto omzet is van alle films in het jaar van uitkomst. Dit doen wij eerst a.d.h.v. een bubble chart, omdat we hiervoor een vergelijking willen maken met drie categorieën (winst, jaar, film), over een tijd/periode. Daarna volgt een bar chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder maken we eerst een kopie van de films dataset. Daarna voegen we een extra kolom aan de nieuwe dataset die de omzet neemt van een film. Ook resetten we de index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Maak een kopie van de films dataset\n",
    "films_revenue = films[[\"title_year\"]].copy()\n",
    "\n",
    "# Voeg een extra kolom toe aan de dataset\n",
    "films_revenue[\"movie_net_revenue\"] = films[\"gross\"] - films[\"budget\"]\n",
    "\n",
    "# Reset de index zodat je makkelijker de gegevens van de filmtitel kan krijgen\n",
    "films_revenue.reset_index(inplace=True)\n",
    "films_revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *N.B.: om de omzet te kunnen berekenen zijn er slecht twee kolommen beschikbaar; vandaar dat we de omzet - het budget gebruiken.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu we de dataset hebben met de benodigde gegevens, kunnen we onze chart maken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een bubble chart, waarbij je elke film indeeld in een subgroep als het ware\n",
    "revenue_points = hv.Points(films_revenue, kdims=[\"title_year\", \"movie_net_revenue\"])\n",
    "\n",
    "# Style de plot\n",
    "revenue_points.opts(color=\"movie_title\",\n",
    "                   cmap=\"Category20\",\n",
    "                   line_color=\"black\",\n",
    "                   size=10,\n",
    "                   padding=0.1,\n",
    "                   width=1000,\n",
    "                   height=500,\n",
    "                   title=\"Netto omzet van films\",\n",
    "                   legend_position=\"right\")\n",
    "\n",
    "revenue_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals te zien is, is deze plot niet echt overzichtelijk en veel te groot. Daarom gaan we voor elk jaar een minimum aantal films nemen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korte uitleg\n",
    "max_jaar = films_revenue[\"title_year\"].max()\n",
    "min_jaar = films_revenue[\"title_year\"].min()\n",
    "print(\"Jaar eerste film: \" + str(min_jaar))\n",
    "print(\"Jaar laatste film: \" + str(max_jaar))\n",
    "print(\"Er zitten \" + str(max_jaar - min_jaar) + \" jaren tussen deze twee jaartallen\")\n",
    "print(\"Hierbij nemen we \" + str((max_jaar - min_jaar) * 5) + \" films, met voor elk jaar 5 films\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_year = films_revenue[\"title_year\"]\n",
    "\n",
    "# Maak een functie die ervoor zorgt dat er in elk jaar 5 random films worden genomen\n",
    "def get_random_movies():\n",
    "    if title_year == 2016.0:\n",
    "        movie_title = films_revenue[\"movie_title\"]\n",
    "    \n",
    "    return movie_title\n",
    "\n",
    "\n",
    "get_random_movies()\n",
    "\n",
    "# def get_movies_in_year_2016():\n",
    "#     \"\"\" Toont de films die uit zijn gekomen in 2016 \"\"\" \n",
    "#     for movie in title_year:\n",
    "#         if movie[0:5] == 2016:\n",
    "#             return movie\n",
    "\n",
    "    \n",
    "# get_movies_in_year_2016()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als extraatje kunt u hieronder wél voor elke film de netto omzet bekijken. Hierbij is er een bar chart gemaakt, waarbij de negatieve en positieve waardes op dezelfde positie blijven. *Het kan dus even wennen zijn om dit goed te interpreteren.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gebruik van ipywidgets\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Maak een dropdown voor films\n",
    "films_widget = ipywidgets.Dropdown(options=films_revenue.movie_title.unique().tolist(),\n",
    "                                  value=\"10 Things I Hate About You\",\n",
    "                                  description=\"Kies een film:\")\n",
    "\n",
    "# Maak een dropdown voor jaartal\n",
    "year_widget = ipywidgets.Dropdown(options=films_revenue.title_year.unique().tolist(),\n",
    "                                  value=2016,\n",
    "                                  description=\"Kies een jaar:\")\n",
    "\n",
    "# Maak een select optie voor de omzet\n",
    "measure_widget = ipywidgets.Select(options=[\"movie_net_revenue\"],\n",
    "                                     value=\"movie_net_revenue\",\n",
    "                                     description=\"Maatstaf:\")\n",
    "\n",
    "def create_bar_chart(film, year, measure):\n",
    "    \"\"\" Funcite die ervoor zorgt dat er een Bar chart wordt gemaakt per geselecteerde film\"\"\"\n",
    "    chart = hv.Bars(films_revenue[films_revenue.movie_title == film], kdims=\"title_year\", vdims=measure)\n",
    "    chart.opts(width=600, height=400)\n",
    "    display(chart.opts(title=f\"{film}\"))\n",
    "    \n",
    "widgets.interact(create_bar_chart, film=films_widget, year=year_widget, measure=measure_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stap 7. Communication\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als laatste stap volgt hier een beschrijving van enkele punten. \n",
    "\n",
    "Allereerst wordt er een beschrijving gegeven van hoe het bouwen van de Data Science-pipline is aangepakt. Daarna wordt er een beschrijving van de methode van dataverzamelingen en -bewerkingen gegeven. <br>Vervolgens wordt de dataopbouw, de analyses en de betekenis van de resultaten besproken. Er wordt concreet antwoord gegeven op de onderzoeksvragen en er volgt een beargumenteerde conclusie. Als laatst zijn er gebruikte bronnen te vinden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aanpak Data Science-pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methode dataverzameling en -bewerking\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataopbouw, analyses en resultaten\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antwoorden onderzoeksvragen\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusie\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronnen\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor het samenvoegen van het gemiddelde van meerdere kolommen: <br> \n",
    "https://stackoverflow.com/questions/34734940/row-wise-average-for-a-subset-of-columns-with-missing-values <br>\n",
    "\n",
    "Voor het correlatieonderzoek: <br>\n",
    "https://canvas.hu.nl/courses/7546/files/folder/slides?preview=765874 & https://canvas.hu.nl/courses/7546/pages/lineaire-regressie?module_item_id=248049 <br>\n",
    "\n",
    "Voor het stylen van de scatter-plot: <br> https://www.youtube.com/watch?v=zZZ_RCwp49g <br>\n",
    "\n",
    "Voor de uitleg van supervised machine learning: <br>\n",
    "https://canvas.hu.nl/courses/7546/files/folder/slides?preview=661608 <br>\n",
    "\n",
    "Voor de uitleg van lineaire regressie: <br> http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm <br>\n",
    "\n",
    "Voor de uitleg van classificatie: <br>\n",
    "https://www.cso.ie/en/methods/classifications/classificationsexplained/ <br>\n",
    "\n",
    "Voor de uitleg van unsupervised machine learning en bijbehorende categorieën: <br>\n",
    "https://canvas.hu.nl/courses/7546/files/folder/slides?preview=661608 <br>\n",
    "\n",
    "Voor het tonen van de film met de meeste omzet: <br>\n",
    "https://stackoverflow.com/questions/15741759/find-maximum-value-of-a-column-and-return-the-corresponding-row-values-using-pan <br>\n",
    "\n",
    "Voor het resetten van de index: <br> \n",
    "https://www.youtube.com/watch?v=OYZNk7Z9s6I <br>\n",
    "\n",
    "Voor het maken van een Scatter (point) plot: <br>\n",
    "https://www.youtube.com/watch?v=TRS8kyOimZI <br>\n",
    "\n",
    "Voor het gebruik van ipywidgets: <br>\n",
    "https://ipywidgets.readthedocs.io/en/latest/user_install.html <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
