{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onderzoek Data Science: Films\n",
    "----\n",
    "\n",
    "Voor de casusopdracht van het vak Data Science moet er een pipeline worden gemaakt, waarbij een toegewezen dataset wordt geanalyseerd. Dit data onderzoek is gebaseerd op de dataset over films en is opgezet door groep 6 van klas V2C. De dataset is toegewezen door de Hogeschool Utrecht en bevat onder andere:\n",
    "- filmgegevens, waaronder duur, genres, taal, land van herkomst, budget en opbrengst;\n",
    "- likes op facebook voor regisseur, hoofdrolspelers, totale cast en de film zelf;\n",
    "- score op IMDB en aantal reviews. \n",
    "\n",
    "\n",
    "#### Hoofdvraag\n",
    "Voor dit onderzoek is er een verplichte onderzoeksvraag, die luidt als volgt: \n",
    "- In hoeverre is de omzet van een film te voorspellen op basis van de populariteit op Facebook en IMDB zelf?\n",
    "\n",
    "#### Onderzoeksvragen\n",
    "Dit onderzoek bevat een aantal onderzoeksvragen die wij zullen behandelen:\n",
    "\n",
    "|Beeordeling                    |Onderzoeksvraag                                                                       |\n",
    "|:------------------------------|:-------------------                                                                  |\n",
    "|Externe dataset                |Hoeveel effect heeft de lengte van een trailer op de omzet van de film?               |\n",
    "|Correlatieonderzoek            |Is een film succesvoller op basis van het aantal likes die een acteur (of acteurs) heeft/hebben op Facebook?                                                                                              | \n",
    "|Supervised machine learning    |In hoeverre is de omzet van een film te voorspellen op basis van de populariteit op Facebook en IMDB zelf?                                                                                                          |\n",
    "|Unsupervised machine learning  |Is een film succesvoller op basis van het aantal likes die een acteur (of acteurs) heeft/hebben op Facebook?                                                                                              |\n",
    "|Interactieve visualisatie      |Wat is de totale winst van alle films per jaar?                                       |\n",
    "\n",
    "#### Z-test\n",
    "Voor de hypothesetoets Z-test moet het volgende gebeuren.\t\n",
    "Een filmcriticus stelt dat de score van Engelstalige films lager is dan gemiddeld.\n",
    "Wij moeten onderzoeken met de dataset of deze filmcriticus gelijk heeft. We nemen een steekproef (met pandas.DataFrame.sample(n=100,random_state=1)) van 100 Engelstalige films en beschouwen de hele dataset als populatie. Ook nemen we als betrouwbaarheid 90%. We gebruiken van de dataset alleen de filmgegevens waarbij zowel de taal (language) als de score (imdb_score) bekend zijn.\n",
    "\n",
    "#### Teamleden\n",
    "Het team bestaat uit de volgende drie personen:\n",
    "- Sebastiaan Jansen\n",
    "- Skott de Koster\n",
    "- Mustafa Toprak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inhoudsopgave\n",
    "---\n",
    "1. Data Collection\n",
    "2. Data Processing\n",
    "3. Data Cleaning\n",
    "4. Data Exploration & Analysis\n",
    "5. Model building\n",
    "6. Visualization\n",
    "7. Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 1: Data collection\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De dataset is aan ons gegeven door de Hogeschool Utrecht als onderdeel van de opdracht. Deze dataset houd een groot CSV bestand in met informatie over films. We hebben dus nu de data verzameld en gaan het nu inlezen.\n",
    "De dataset is opgehaald van de volgende GitHub link: https://github.com/tijmenjoppe/ComputationalModelling-student/tree/master/casus/movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ook maken wij gebruik van een externe dataset. Deze externe dataset bevat een link ID naar de trailers van de films die op YouTube te vinden zijn. De externe dataset is opgehaald van de link: https://grouplens.org/datasets/movielens/20m-youtube/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 2: Data processing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gaan nu de dataset inlezen en verwerken om het beter te kunnen bekijken. Om te beginnen importeren we de benodigde Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deze libraries zijn voor het verwerken van de data.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "\n",
    "# Deze libraries zijn voor het verkrijgen van YouTube video gegevens. Dit is relevant voor het gebruik van de externe dataset.\n",
    "import pafy\n",
    "from youtube_dl import YoutubeDL\n",
    "import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vervolgens lezen we de eerste (toegewezen) dataset in. Hiervoor gebruiken we pandas.read_csv en slaan we dit op in de dataframe \"films\". \n",
    "Aangezien de dataset erg groot is, laten we voor nu alleen de eerste 5 rijen zien. Dit doen we door een variabele films.head() aan te roepen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films = pd.read_csv(\"movie.csv\", sep=\",\")\n",
    "films.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hetzelfde doen we met de tweede (externe) dataset, deze noemen we \"films_extern\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_extern = pd.read_csv(\"ml-youtube.csv\", sep=\",\")\n",
    "films_extern.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien dus dat de eerste dataset uit veel films bestaat. De tweede dataset heeft er nog veel meer, maar niet alle data in de sets is bruikbaar, dus dat gaan we nu schoonmaken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stap 3: Data cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gaan nu de data schoon maken. Om te beginnen schonen we beide datasets op door alle data die niet relevant is te verwijderen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\"color\", \"facenumber_in_poster\", \"country\", \"aspect_ratio\", \"content_rating\"]\n",
    "films.drop(to_drop, inplace=True, axis=1)\n",
    "\n",
    "films_extern.drop(\"movieId\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We verwijderen de rijen met NaN gegevens in beide datasets, aangezien dit data is die we niet kunnen gebruiken.\n",
    "films = films.dropna()\n",
    "films_extern = films_extern.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We halen alle duplicates eruit.\n",
    "films = films.drop_duplicates(subset=\"movie_title\")\n",
    "films_extern = films_extern.drop_duplicates(subset=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We halen een whitespace weg in de eerste dataset die er niet in hoort te zitten.\n",
    "# We slicen de laatste 7 characters van de title column (het jaar dat de film uitkwam) in de tweede dataset weg,\n",
    "# zodat de column overeen komt met de column in de eerste dataset.\n",
    "films.update(films[\"movie_title\"].str[:-1])\n",
    "films_extern.update(films_extern[\"title\"].str[:-7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We setten de indexes van beide datasets naar de titel van de films.\n",
    "films.set_index(\"movie_title\", inplace=True)\n",
    "films_extern.set_index(\"title\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sorteren beide datasets zodat de waardes gelijk zullen zijn.\n",
    "films = films.sort_index()\n",
    "films_extern = films_extern.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu ziet de eerste dataset er zo uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ziet de tweede dataset er zo uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_extern.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu gaan we beide datasets samenvoegen tot 1 dataframe, deze noemen we 'films_extern_merge'. Het idee is om de youtubeId column van de tweede dataset te verwerken in de eerste dataset. Deze gemergde dataset gaan we later gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omdat de youtubeId column het enige is wat relevant is, is dat ook het enige veld dat we toevoegen. \n",
    "# De indexes van de eerste en tweede dataset worden samengevoegd tot 1 index genaamd title.\n",
    "films_extern_merge = pd.merge(films, films_extern, left_index=True, right_index=True)\n",
    "films_extern_merge.index.name = \"title\"\n",
    "\n",
    "# Tot slot droppen we de niet-relevante kollomen. Om code te besparen, overschrijven we alleen de dataframe met de relevante kollomen.\n",
    "films_extern_merge = films_extern_merge[[\"gross\", \"youtubeId\"]]\n",
    "films_extern_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kunt zien dat er 2827 films zijn. Dit is bijna de helft minder dan er in de oorspronkelijke eerste dataset stond, dit is omdat we de onbruikbare data hebben verwijderd en er alleen bruikbare data is overgebleven.\n",
    "We gaan de gemergede dataset nu verkennen en analyseren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stap 4: Data exploration & analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de data te verkennen en er een betere grip op te krijgen zullen wij een paar commando's loslaten op de verwerkte data. Als eerste zullen wij een .describe gebruiken om een overzicht te krijgen van alle info van de numerieke waardes. Op deze manier kunnen wij bijvoorbeeld de gemiddelde lengte van een film zien. Dit is bij deze dataset dus 105 minuten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "films.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de data verder te verkennen en een beter gevoel te krijgen kunnen wij gebruik maken van grafieken. Zo staat hieronder bijvoorbeeld een staafdiagram van het aantal films per jaartal. In de x as staat het jaartal, deze zijn gesorteerd bij hoeveel films er in dat jaartal zijn uitgekomen die in deze dataset staan. De meeste films uit deze dataset komen dus uit 2002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "films[\"title_year\"].value_counts().plot(kind=\"bar\", width=0.5,figsize=(20,5));\n",
    "plt.xlabel(\"Jaartal\", labelpad=14)\n",
    "plt.ylabel(\"Aantal films\", labelpad=14)\n",
    "plt.title(\"Aantal films per jaartal\", y=1.02);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onderzoeksvragen\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor deze notebook moeten er verschillende onderzoeksvragen worden uitgewerkt, elk met hun eigen antwoord en andere manier van oplossen. Deze zullen nu één voor één worden uitgewerkt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vraag 1: Hoeveel effect heeft de lengte van een trailer op de omzet van de film?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trailers zijn een belangrijk onderdeel van films. Je wilt immers wel enigzins weten wat je kan verwachten van de film.<br>\n",
    "Dit kan ook een factor spelen in de omzet van de film. Want als de trailer slecht is zal je waarschijnlijk ook niet snel naar de film gaan, en dan zal de film dus ook minder omzet hebben.<br>\n",
    "We gebruiken hier alleen de lengte van de trailer, om het simpel te houden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor deze vraag maken we gebruik van de gemergde dataset uit stap 3. Zoals je misschien al hebt kunnen raden bestaat de externe dataset uit ID's voor YouTube video's. Deze verwijzen naar de trailers van de films van de toegewezen dataset. We hebben bij stap 3 de toegewezen en externe dataset gemerged tot 1 dataset, deze gaan we nu gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_extern_merge.iloc[:,-1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier komt de pafy library ter zake. Om een voorbeeld te geven over wat het kan doen maken we eerst een video variabele aan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Om het simpel te houden gebruiken we voor nu maar de youtubeId van de eerste film in de dataframe.\n",
    "url = films_extern_merge.iloc[0, -1]\n",
    "video = pafy.new(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Van deze video kunnen we nu verschillende data ophalen, hier een paar voorbeelden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De titel van de video\n",
    "print(\"De titel van de video is: \\n\" + str(video.title))\n",
    "print(\"\\n\")\n",
    "\n",
    "# De lengte van de video in secondes\n",
    "print(\"De lengte van de video in secondes is: \\n\" + str(video.length))\n",
    "print(\"\\n\")\n",
    "\n",
    "# De concrete duratie van de video\n",
    "print(\"De concrete duratie van de video is: \\n\" + str(video.duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er zijn nog veel andere statistieken die kunnen worden opgehaad met behulp van pafy, maar hiervoor is een YouTube API account voor nodig en is ook niet voor ons relevant. We focussen ons op de <b>concrete duratie</b> van de video.\n",
    "\n",
    "Dus, heeft de duratie van de trailer invloed op de omzet van de film? Om daar achter te komen maken we nieuwe dataframe aan, deze pakt 100 willekeurige films uit de dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# De dataframe heet \"random_films\".\n",
    "random_films = films_extern_merge.sample(n = 100)\n",
    "# In deze list slaan we zometeen de duraties op.\n",
    "duratie = []\n",
    "# We loopen door random_films heen en extracten van elke video de duratie in secondes.\n",
    "# LET OP! Dit duurt een tijdje, aangezien er elke keer opnieuw een HTTP request wordt gemaakt. Je kunt ook de errors en warnings negeren.\n",
    "for x in random_films[\"youtubeId\"].values:\n",
    "    try:\n",
    "        # We appenden de lengte van de trailer toe aan de list.\n",
    "        duratie.append(pafy.new(x).length)\n",
    "    except Exception:\n",
    "        # Sommige video's zijn niet meer te zien op YouTube, hier kan de data dus ook niet van worden opgehaald.\n",
    "        # Maar aangezien de waardes niet door elkaar mogen lopen, appenden we een Null waarde.\n",
    "        duratie.append(None)\n",
    "        continue\n",
    "\n",
    "# We voegen een kolom toe met de waardes van duratie.\n",
    "random_films[\"trailer_length\"] = duratie\n",
    "# We halen alle trailers eruit met een duratie van meer dan 400 seconden, aangezien deze vaak niet kloppen.\n",
    "random_films = random_films[random_films.trailer_length < 400]\n",
    "random_films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu maken we een mooie scatterplot om te zien of er een verband is tussen de omzet en trailer duratie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=random_films[\"trailer_length\"], y=random_films[\"gross\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elke uitkomst die je krijgt zal een ander resultaat geven. Maar wat wel duidelijk is in elke uitkomst is dat er geen specifiek partroon te zien is.\n",
    "<br>\n",
    "**Conclusie:** We kunnen dus concluderen dat er <b>geen</b> duidelijk verband is tussen de lengte van de trailer en de omzet van de film."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vraag 2: Is een film succesvoller op basis van het aantal likes die een acteur (of acteurs) heeft/hebben op Facebook?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social media speelt tot op de dag van vandaag een hele belangrijke rol in ons dagelijks leven. Overal zien we beroemdheden en sommige mensen willen heel graag op deze beroemdheden lijken. Hierdoor wordt het gedrag van een individu bepaald. \n",
    "\n",
    "Tegenwoordig draaien de meeste dingen om likes en dat geldt ook voor deze deelvraag. Is het echt waar dat een film succesvoller is op basis van het aantal likes die een acteur (of acteurs) heeft/hebben op Facebook?\n",
    "\n",
    "Om die vraag te kunnen beantwoorden, moeten we eerst twee beoordelingscriteria langs. Het eerste criterium zal gaan over het correlatieonderzoek. Bij het tweede criterium wordt er gebruik gemaakt van een **unsupervised machine learning** techniek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlatieonderzoek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met behulp van correlatie wordt de samenhang gemeten tussen twee variabelen. \n",
    "\n",
    "Het is belangrijk dat je de waarde van een andere variabele goed kunt voorspellen o.b.v. waardes van een variabele die je al kent. Hoe groter de correlatie, des te hoger de voorspellingskracht is.<br>\n",
    "Om deze vraag te kunnen beantwoorden, worden de variabelen IMDB-score en het gemiddelde aantal likes van de acteurs genomen.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder maken we eerst een extra kolom aan die het gemiddelde neemt van het aantal likes van de acteurs. Deze kolom voegen we toe aan de dataset films:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Voeg een kolom van het gemiddeld aantal likes van de acteurs toe aan de dataset films.\n",
    "films[\"mean_likes_actors\"] = (films[\"actor_1_facebook_likes\"] + \n",
    "                              films[\"actor_2_facebook_likes\"] + \n",
    "                              films[\"actor_3_facebook_likes\"]) / 3\n",
    "films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals gezegd hebben de worden de variabelen IMDB-score en mean_likes_actors gebruikt. Hieronder wordt een scatter-plot gemaakt van deze variabelen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak gebruik van de Seaborn style.\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "# Maak een scatter-plot van de variabelen.\n",
    "plt.scatter(films.imdb_score, films.mean_likes_actors, \n",
    "            s=40, \n",
    "            c=\"red\", edgecolor=\"black\", \n",
    "            linewidth=0.75, alpha=0.75)\n",
    "\n",
    "# Voeg kenmerken toe aan de plot.\n",
    "plt.xlabel(\"IMDB-score\")\n",
    "plt.ylabel(\"Aantal likes\")\n",
    "plt.title(\"Correlatie tussen IMDB-score en aantal likes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals te zien is liggen de punten heel dicht op elkaar. Nu laten we nog even de r-waardes (richtingscoëfficient) zien van de correlatie tussen deze twee variabelen. Hiervoor maken we een aparte dataset aan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Voeg de kolommen imdb_score en mean_likes_actors toe aan een nieuwe dataset.\n",
    "films_correlation = films[[\"imdb_score\", \"mean_likes_actors\"]]\n",
    "films_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toon de correlatie tussen de twee variabelen.\n",
    "films_correlation.corr().style.background_gradient(cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We hebben een aparte dataset aangemaakt voor deze correlatie, omdat de andere gegevens niet relevant zijn voor het beantwoorden van de vraag. Mocht u geïntereseerd zijn in de correlatie met alle variabelen, kunt u het onderstaande statement runnen:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geef een overzicht van de correlatie met alle variabelen.\n",
    "# films.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor een goede correlatie is het belangrijk dat de richtingscoëfficient dicht bij de 1 zit ($ -1 ≤ r ≤ 1$). We zien uit de bovenstaande gegevens dat de richtingscoëfficient voor imdb_score en mean_likes_actors 0.187995 is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er is sprake van een positieve correlatie, maar er is **geen** sprake van een sterk verband tussen het succes van een film en het aantal likes van acteurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vraag 3: In hoeverre is de omzet van een film te voorspellen op basis van de populariteit op Facebook en IMDB zelf?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omzet is erg belangrijk voor een film. Het is immers waar een film voor wordt gemaakt: om omzet te draaien. Deze omzet komt natuurlijk van alle mensen die deze film hebben gezien in de bioscoop en uiteindelijk ook de DVD's. <br>\n",
    "Maar natuurlijk gaat de gemiddelde persoon niet naar elke film die ooit uitkomt. Als een film een lage score heeft op IMDB of weinig Facebook likes heeft, geeft dat al gauw een signaal dat de film niet zo goed is. Het is dus veilig om te zeggen dat zo'n score invloed heeft op de populariteit van een film. <br>\n",
    "Maar heeft dit ook veel effect op de omzet van de film? En tot hoeverre kunnen we hiermee de omzet van toekomstige films voorspellen? Dat gaan we nu uitzoeken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor deze vraag wordt gebruikt gemaakt van <b>supervised machine learning.</b> Dit is gebaseerd op een dataset waarvan de uitkomst al bekend is en houdt in dat we de machine letterlijk \"begeleiden\" in zijn taak om iets uit te voeren. We geven de machine data om te onderzoeken en zorgen dat we deze een goed antwoord kan geven.<br>\n",
    "Onder supervised machine learning worden er binnen het vak Data Science twee technieken verstaan:\n",
    "    \n",
    "|Techniek                       |Omschrijving                          |                                \n",
    "|:------------------------------|:-------------------------------------|\n",
    "|Lineaire regressie             |Lineaire regressie probeert de relatie tussen twee variabelen te modelleren door een <b>lineaire vergelijking</b> aan te passen aan de waargenomen gegevens.|\n",
    "|Classificatie                  |Classificatie is een geordende reeks gerelateerde categorieën die wordt gebruikt om gegevens te <b>groeperen op basis van overeenkomsten</b>. Binnen het vak Data Science bevat het 2 onderdelen: <b>decision trees</b> en <b>K-nearest neighbors</b>.                                            |\n",
    "\n",
    "Om een betrouwbaar onderzoek uit te voeren maken we gebruik van <b>beide</b> technieken. We beginnen met lineaire regressie, dan classificatie. Maar voordat we beginnen moeten we een paar dingen instellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We maken een kopie van de films dataset met alleen de relevante data.\n",
    "films_sml = films[[\"imdb_score\", \"movie_facebook_likes\", \"gross\"]].copy()\n",
    "\n",
    "# We stellen de variabele X op, deze vertegenwoordigd de feature matrix (X-as).\n",
    "# Het bevat de totale filmscore op IMDB en de het totale aantal facebook likes per film.\n",
    "feature_cols = [\"imdb_score\",\"movie_facebook_likes\"]\n",
    "\n",
    "X = films_sml[feature_cols]\n",
    "\n",
    "# We stellen we de variabele Y op, deze vertegenwoordigd de target vector(Y-as).\n",
    "# Het bevat de totale omzet per film.\n",
    "Y = films_sml.gross\n",
    "\n",
    "# We splitten de data op in test en training sets. 75% van de data gaat naar de training set, 25% gaat naar de test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=1)\n",
    "\n",
    "# We printen de shape van de training sets.\n",
    "print(\"De shape van de X train set is \" + str(X_train.shape))\n",
    "print(\"De shape van de Y train set is \" + str(Y_train.shape))\n",
    "print(\"De shape van de X test set is \" + str(X_test.shape))\n",
    "print(\"De shape van de Y test set is \" + str(Y_test.shape))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principe hebben we nu de eerste paar stappen van de werkwijze van Scikit-learn toegespast. De variabelen kunnen we hergebruiken in beide technieken. Zo hoeven we ze niet telkens opnieuw aan te maken.<br>\n",
    "Nu we de voorbereidingen hebben gemaakt, kunnen we beginnen met lineaire regressie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We importeren het benodigde onderdeel van scikit-learn.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# We instantieeren de lineaire regressie.\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# We fitten het model aan de training data, oftewel we leren de coeffienten.\n",
    "linreg.fit(X_train, Y_train)\n",
    "\n",
    "# We printen the onderschepping en coëfficiënten.\n",
    "print(\"De onderschepping van de lineaire regressie is \" + str(linreg.intercept_))\n",
    "print(\"De coëfficiënt van de lineaire regressie is \" + str(linreg.coef_))\n",
    "print(\"\\n\")\n",
    "\n",
    "# We paren de coëfficiënten met de feature namen.\n",
    "list(zip(feature_cols, linreg.coef_))\n",
    "\n",
    "# We maken voorspellingen op de test set.\n",
    "Y_pred = linreg.predict(X_test)\n",
    "\n",
    "# Tot slot berekenen we de RMSE (Root Mean Squared Error) voor de omzet voorspellingen.\n",
    "print(\"De RMSE van de omzet voorspellingen is \" + str(np.sqrt(metrics.mean_squared_error(Y_test, Y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze nummers zeggen niet veel duidelijks. We willen de RMSE namelijk zo laag mogelijk hebben. Dit kunnen we proberen om waar te maken door de code opnieuw te draaien, maar met 1 feature in plaats van 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er is hier een mogelijk verklaring voor: <b>correlatie.</b> Je hebt immers voor lineaire regressie enige <b>correlatie nodig</b>. Om te checken of er correlatie is doen we het volgende:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_sml.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien dat de correlatie tussen de feature matrix en target vector erg klein is. Voor het correct uitvoeren van lineaire regressie is een minimale correlatie nodig van <b>0.7</b> (positief) of <b>-0.7</b> (negatief)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als we nu een aantal scatterplots opstellen tussen de feature matrix en target vector kunnen we zien waarom er weinig correlatie is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(films_sml, x_vars=[\"imdb_score\",\"movie_facebook_likes\"], y_vars=\"gross\", height=7, aspect=0.9, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In beide scatterplots is geen goede correlatie te zien. We kunnen nu dus concluderen dat er met lineaire regressie <b>geen</b> goede voorspelling kan worden gemaakt. <br>\n",
    "\n",
    "Maar hoe zit het met classificatie? Dat gaan we nu uitzoeken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals eerder vermeld vallen er binnen het vak Data Science 2 technieken onder classificatie: **Decision Trees** en **k-Nearest Neighbours**. Binnen deze context kunnen we alleen gebruik maken van Decision Trees. De techniek k-Nearest Neighbours is namelijk voor het indelen van een **nieuw object**, wat in principe niet kan bijdragen aan het beantwoorden van de vraag.\n",
    "\n",
    "Decision trees is een algoritme dat telkens een vraag bedenkt om de bestaande objecten in te delen in groepen, totdat deze uiteindelijk nauwkeurig zijn ingedeeld. Er wordt mee voorspeld wat het type is van verschillende cultivars, bijvoorbeeld een appel als je fruit gaat onderzoeken.<br>\n",
    "In dit geval zal het proberen om de types IMDB score of facebook likes te vinden.<br>\n",
    "Laten we beginnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We importeren het benodigde onderdeel van scikit-learn.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# We instantieeren de decision tree classifier.\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# We fitten de decision tree op de training sets.\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "\n",
    "# We voorspellen hoe nauwkeurig het algoritme de type van de cultivars kan voorspellen.\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(\"Nauwkeurigheid:\", metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er is een 0.0 in nauwkeurigheid. Dit is heel slecht, want dit laat zien dat het algoritme de types absoluut niet kan voorspellen. Dit kunnen we vaststellen door Graphviz te gebruiken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dit is een fix voor een error met graphviz.\n",
    "# Het zorgt ervoor dat de error \"InvocationException: GraphViz's executables not found\" niet voorkomt.\n",
    "import os\n",
    "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \";\" + os.environ[\"CONDA_PREFIX\"] + r\"\\Library\\bin\\graphviz\"\n",
    "\n",
    "# We importeren de benodigde libraries.\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG\n",
    "\n",
    "# We genereren een graph met de decision tree en visualeren die.\n",
    "graph = Source(tree.export_graphviz(clf, out_file=None, feature_names=X.columns))\n",
    "SVG(graph.pipe(format=\"svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze graphic is absoluut veel te groot en je wordt er niet wijzer uit. We kunnen dus concluderen dat ook met decision trees de omzet **niet goed voorspeld kan worden.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wat kan de reden zijn dat de omzet niet goed voorspeld kan worden? Als we nog een keer naar de scatterplots kijken..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(films_sml, x_vars=[\"imdb_score\",\"movie_facebook_likes\"], y_vars=\"gross\", height=7, aspect=0.9, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dan zien we dat imdb_score en movie_facebook_likes compleet andere eigenschappen hebben. De IMDB score is een getal van 1 tot 10, terwijl de facebook likes kunnen lopen tot in de 300.000. De 2 variabelen verschillen te veel van elkaar en hebben te weinig correlatie voor een goede voorspelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De omzet kan **niet** goed worden voorspeld met supervised machine learning, omdat de variabelen te veel van elkaar verschillen en er te weinig correlatie is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stap 7. Communication\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als laatste stap volgt hier een beschrijving van enkele punten. \n",
    "\n",
    "Allereerst wordt er een beschrijving gegeven van hoe het bouwen van de Data Science-pipline is aangepakt. Daarna wordt er een beschrijving van de methode van dataverzamelingen en -bewerkingen gegeven. <br>Vervolgens wordt de dataopbouw, de analyses en de betekenis van de resultaten besproken. Er wordt concreet antwoord gegeven op de onderzoeksvragen en er volgt een beargumenteerde conclusie. Als laatst zijn er gebruikte bronnen te vinden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aanpak Data Science-pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methode dataverzameling en -bewerking\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataopbouw, analyses en resultaten\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antwoorden onderzoeksvragen\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusie\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronnen\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor het samenvoegen van het gemiddelde van meerdere kolommen:<br> \n",
    "https://stackoverflow.com/questions/34734940/row-wise-average-for-a-subset-of-columns-with-missing-values<br>\n",
    "Voor het correlatieonderzoek:<br>\n",
    "https://canvas.hu.nl/courses/7546/files/folder/slides?preview=765874 & https://canvas.hu.nl/courses/7546/pages/lineaire-regressie?module_item_id=248049 <br>\n",
    "Voor het stylen van de scatter-plot:<br> https://www.youtube.com/watch?v=zZZ_RCwp49g <br>\n",
    "Voor de uitleg van lineaire regressie:<br> http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm <br>\n",
    "Voor de uitleg van classificatie:<br>\n",
    "https://www.cso.ie/en/methods/classifications/classificationsexplained/   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
